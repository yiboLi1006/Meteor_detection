Training Log
================================================================================
Started training on device: cuda
Timestamp: 0821_1923
Number of epochs: 200
Model: OptimizedMeteorCNN
Batch size: 32
Initial learning rate: 0.0002
Loss function: FocalLoss
Early stopping patience: 12
================================================================================
Epoch, Train Loss, Train Accuracy, Val Loss, Val Accuracy, Learning Rate, Best Model Saved, Time
1, 0.084061, 0.729706, 0.053024, 0.818404, 0.00020000, True, 251.11
2, 0.048114, 0.825286, 0.044908, 0.844647, 0.00020000, True, 381.35
3, 0.041157, 0.860450, 0.040286, 0.868964, 0.00020000, True, 381.15
4, 0.035549, 0.884010, 0.064217, 0.749475, 0.00020000, False, 379.66
5, 0.032798, 0.895615, 0.025426, 0.923548, 0.00020000, True, 380.32
6, 0.031801, 0.900047, 0.027454, 0.901330, 0.00020000, False, 381.54
7, 0.028082, 0.910194, 0.024782, 0.932645, 0.00020000, True, 379.31
8, 0.026250, 0.917425, 0.031370, 0.893107, 0.00020000, False, 381.90
9, 0.025902, 0.918125, 0.027651, 0.916375, 0.00020000, False, 381.33
10, 0.024955, 0.922557, 0.120845, 0.699265, 0.00020000, False, 380.33
11, 0.023304, 0.926872, 0.018677, 0.950140, 0.00020000, True, 382.42
12, 0.021726, 0.932820, 0.014157, 0.955038, 0.00020000, True, 381.25
13, 0.020273, 0.937777, 0.123450, 0.696291, 0.00020000, False, 380.97
14, 0.019211, 0.941451, 0.511438, 0.511022, 0.00020000, False, 382.64
15, 0.020014, 0.939235, 0.013988, 0.963086, 0.00020000, True, 382.43
16, 0.016818, 0.950198, 0.021867, 0.935794, 0.00020000, False, 381.99
17, 0.017414, 0.946174, 0.207883, 0.787964, 0.00020000, False, 383.37
18, 0.015804, 0.952881, 0.741477, 0.474108, 0.00020000, False, 382.02
19, 0.017221, 0.947166, 0.011494, 0.969909, 0.00020000, True, 382.08
20, 0.015521, 0.954630, 0.975079, 0.436319, 0.00020000, False, 376.69
21, 0.015353, 0.954047, 0.025410, 0.910602, 0.00020000, False, 388.10
22, 0.013884, 0.959937, 0.286619, 0.595346, 0.00020000, False, 387.41
23, 0.012874, 0.963436, 0.015810, 0.944367, 0.00020000, False, 392.39
24, 0.010574, 0.970376, 0.135041, 0.638733, 0.00010000, False, 388.29
25, 0.009867, 0.973000, 0.008469, 0.979706, 0.00010000, True, 386.30
26, 0.010064, 0.971075, 0.172864, 0.629111, 0.00010000, False, 389.68
27, 0.009346, 0.973991, 0.005691, 0.988279, 0.00010000, True, 388.38
28, 0.008246, 0.976324, 2.873939, 0.612141, 0.00010000, False, 390.19
29, 0.007937, 0.978365, 0.475344, 0.537089, 0.00010000, False, 389.05
30, 0.007901, 0.977373, 0.005589, 0.988104, 0.00010000, True, 388.72
31, 0.007572, 0.978015, 0.019426, 0.952589, 0.00010000, False, 390.07
32, 0.007197, 0.979706, 0.006276, 0.986529, 0.00010000, False, 391.23
33, 0.006253, 0.980989, 0.017785, 0.949790, 0.00010000, False, 386.80
34, 0.006930, 0.980231, 0.312226, 0.520819, 0.00010000, False, 392.92
35, 0.005544, 0.985304, 0.005173, 0.983205, 0.00005000, True, 388.55
36, 0.004924, 0.986121, 0.003301, 0.990903, 0.00005000, True, 389.08
37, 0.004645, 0.986762, 0.004562, 0.987579, 0.00005000, False, 389.57
38, 0.004322, 0.986529, 0.067219, 0.814206, 0.00005000, False, 390.50
39, 0.003633, 0.990378, 0.183226, 0.836599, 0.00005000, False, 389.48
40, 0.004798, 0.986179, 0.003825, 0.989328, 0.00005000, False, 391.74
41, 0.003621, 0.990436, 0.003668, 0.990378, 0.00002500, False, 390.29
42, 0.003195, 0.990261, 0.002243, 0.992652, 0.00002500, True, 388.15
43, 0.003214, 0.990436, 0.014721, 0.950140, 0.00002500, False, 390.91
44, 0.002981, 0.990844, 0.002374, 0.993877, 0.00002500, False, 392.77
45, 0.002485, 0.992477, 0.005126, 0.984780, 0.00002500, False, 393.32
46, 0.002695, 0.992186, 0.002285, 0.992302, 0.00002500, False, 389.82
47, 0.002486, 0.993235, 0.001992, 0.993177, 0.00001250, True, 388.12
48, 0.002413, 0.992652, 0.001935, 0.994577, 0.00001250, True, 393.86
49, 0.002464, 0.993002, 0.013741, 0.974283, 0.00001250, False, 390.59
50, 0.002326, 0.992944, 0.002731, 0.991777, 0.00001250, False, 386.91
51, 0.002165, 0.993469, 0.001814, 0.994927, 0.00001250, True, 389.45
52, 0.002052, 0.994110, 0.003826, 0.987929, 0.00001250, False, 392.47
53, 0.001904, 0.994168, 0.004109, 0.988453, 0.00001250, False, 389.75
54, 0.002022, 0.994227, 0.002026, 0.994402, 0.00001250, False, 388.25
55, 0.002105, 0.993352, 0.003375, 0.990728, 0.00001250, False, 392.19
56, 0.001811, 0.994110, 0.002005, 0.994052, 0.00000625, False, 388.45
57, 0.001719, 0.995218, 0.001754, 0.995276, 0.00000625, True, 391.71
58, 0.001605, 0.994927, 0.001694, 0.995101, 0.00000625, True, 383.79
59, 0.001691, 0.995160, 0.001664, 0.995801, 0.00000625, True, 391.36
60, 0.001748, 0.994927, 0.001639, 0.994752, 0.00000625, True, 389.00
61, 0.001626, 0.994985, 0.001614, 0.994227, 0.00000625, True, 388.48
62, 0.001854, 0.994635, 0.001658, 0.994752, 0.00000625, False, 388.80
63, 0.001769, 0.994810, 0.001618, 0.995451, 0.00000625, False, 388.88
64, 0.001439, 0.996035, 0.001556, 0.995626, 0.00000625, True, 388.91
65, 0.001554, 0.994810, 0.001578, 0.994752, 0.00000625, False, 389.59
66, 0.001406, 0.995743, 0.001863, 0.993877, 0.00000625, False, 387.30
67, 0.001262, 0.995918, 0.001956, 0.994402, 0.00000625, False, 389.64
68, 0.001359, 0.995568, 0.001567, 0.995276, 0.00000625, False, 389.40
69, 0.001468, 0.995743, 0.001482, 0.995101, 0.00000313, True, 387.32
70, 0.001442, 0.995918, 0.001735, 0.994927, 0.00000313, False, 386.96
71, 0.001299, 0.995743, 0.001797, 0.994752, 0.00000313, False, 387.28
72, 0.001564, 0.995335, 0.001638, 0.995101, 0.00000313, False, 385.44
73, 0.001201, 0.996209, 0.001911, 0.993527, 0.00000313, False, 391.70
74, 0.001364, 0.995976, 0.001520, 0.995101, 0.00000156, False, 388.64
75, 0.001253, 0.996793, 0.001778, 0.994402, 0.00000156, False, 386.93
76, 0.001475, 0.995510, 0.001861, 0.994227, 0.00000156, False, 390.34
77, 0.001498, 0.995743, 0.001466, 0.995276, 0.00000156, True, 389.77
78, 0.001263, 0.995918, 0.001780, 0.994402, 0.00000156, False, 386.11
79, 0.001174, 0.996443, 0.001592, 0.994927, 0.00000156, False, 389.69
80, 0.001341, 0.995860, 0.001814, 0.994402, 0.00000156, False, 388.85
81, 0.001464, 0.995626, 0.001464, 0.994577, 0.00000156, True, 387.75
82, 0.001302, 0.995860, 0.001806, 0.993527, 0.00000156, False, 389.62
83, 0.001200, 0.996559, 0.001787, 0.994402, 0.00000156, False, 391.48
84, 0.001274, 0.996443, 0.001577, 0.994927, 0.00000156, False, 390.58
85, 0.001120, 0.996209, 0.001606, 0.994577, 0.00000156, False, 392.16
86, 0.001397, 0.995626, 0.001464, 0.995451, 0.00000100, False, 389.31
87, 0.001219, 0.996151, 0.001886, 0.994402, 0.00000100, False, 390.25
88, 0.001387, 0.995451, 0.001486, 0.994927, 0.00000100, False, 385.28
89, 0.001140, 0.996734, 0.001814, 0.994577, 0.00000100, False, 391.42
90, 0.001246, 0.996443, 0.001482, 0.995626, 0.00000100, False, 389.58
91, 0.001212, 0.996268, 0.001427, 0.995976, 0.00000100, True, 387.12
92, 0.001120, 0.997143, 0.001427, 0.995801, 0.00000100, False, 387.91
93, 0.001392, 0.995568, 0.001545, 0.994752, 0.00000100, False, 388.51
94, 0.001333, 0.996151, 0.001485, 0.995276, 0.00000100, False, 386.89
95, 0.001196, 0.995918, 0.001426, 0.995451, 0.00000100, True, 392.33
96, 0.001250, 0.995743, 0.001401, 0.995451, 0.00000100, True, 386.63
97, 0.001079, 0.996209, 0.001442, 0.996151, 0.00000100, False, 389.86
98, 0.001263, 0.996501, 0.001482, 0.995276, 0.00000100, False, 385.35
99, 0.001118, 0.997376, 0.001770, 0.993702, 0.00000100, False, 386.94
100, 0.001139, 0.996443, 0.001457, 0.995801, 0.00000100, False, 389.08
101, 0.001124, 0.996501, 0.001631, 0.995101, 0.00000100, False, 390.01
102, 0.000936, 0.997259, 0.001503, 0.995451, 0.00000100, False, 392.17
103, 0.001064, 0.996734, 0.001602, 0.994752, 0.00000100, False, 388.33
104, 0.001212, 0.995801, 0.001589, 0.994752, 0.00000100, False, 386.54
105, 0.001162, 0.996618, 0.001450, 0.995626, 0.00000100, False, 387.23
106, 0.000949, 0.996443, 0.001509, 0.995101, 0.00000100, False, 393.60
107, 0.001102, 0.997084, 0.001419, 0.995801, 0.00000100, False, 391.25
Early stopping triggered at epoch 108
================================================================================
Training completed after 108 epochs
Best validation loss: 0.001401
Final train accuracy: 0.996851
Final validation accuracy: 0.994927
Training log completed
